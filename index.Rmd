---
title: "King and Queen of pop"
author: "Bas de Boer"
date: "2/16/2022"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
    
---

```{r setup, include=FALSE}


library(tidyverse)
library(spotifyr)
library(forcats)
library(plotly)
library(flexdashboard)
library(compmus)
library(reshape)

corpus_king <- get_playlist_audio_features("", "37i9dQZF1DXaTIN6XNquoW")
corpus_queen <- get_playlist_audio_features("", "37i9dQZF1DWTQllLRMgY9S")

best_track_king_billie <- get_track_audio_features(c("2IU9ftOgyRL2caQGWK1jjX", "5ChkMS8OtdzJeqyybCc9R5"))
best_track_queen_vergin <- get_track_audio_features(c("1C2h7mLntPSeVYciMRTF4a", "1ZPlNanZsJSPK5h9YZZFbZ"))
  
combined_corpus <-
  bind_rows(
    corpus_king %>% mutate(category = "King of pop (Michael Jackson)"),
    corpus_queen %>% mutate(category = "Queen of pop (Madonna)")
  )

```

``` {r get_means_mj, include=FALSE}
mean_king <- corpus_king %>%
  summarise(
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_instrumentalness = mean(instrumentalness),
    mean_liveness = mean(liveness),
    mean_valence = mean(valence)
    
  )
```

``` {r get_means_madon, include=FALSE}

mean_queen <- corpus_queen %>%
  summarise(
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_instrumentalness = mean(instrumentalness),
    mean_liveness = mean(liveness),
    mean_valence = mean(valence)
    
  )
```

```{r key-and-chord, include= FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

```

### Introduction page

The corpus that will be used in this research consists out of the two Spotify "This is:" playlists. The first playlist is the "This Is Micheal Jackson" playlist (containing 65 tracks), the second playlist is the "This Is Madonna" playlist (containing 59 tracks). These playlists are curated by Spotify’s Shows & Editorial team. This is a team of music experts and genres specialists from around the world, hired by Spotify to curate and manage Spotify’s own playlists. 

So, the king and the queen of pop. I choose this corpus because I think both these artists where so good in the music that they made that they became legends, not only in the pop-genre, but for almost every music-lover all over the world. Regardless of the country or age of a person, everybody knows them. The playlist that will be used are on the right side of this page.

This Is Micheal Jackson playlist: https://open.spotify.com/playlist/37i9dQZF1DXaTIN6XNquoW

This Is Madonna playlist: https://open.spotify.com/playlist/37i9dQZF1DWTQllLRMgY9S


The groups that will be compared in this corpus are the artists. What are the features that these two artist have in common? Could this be the features that made them become legends? But also; How do songs of the king and queen of pop defer from each other? In the field of danceability, energy, tempo, instrumentalness, speechiness and more...



One of the possible problems may be in the fact that in the "This is: Micheal jackson" playlist there are also songs of the Jackson five. These songs where of course also iconic for Micheal Jackson, however, they may be a bit less representative for the overall Micheal Jackson playlist. However, there is also a possibility that from these tracks new insights will be derived.

Typical:
Micheal Jackson: "Billie jean", "Smooth criminal" Madonna: "Die another day", "Papa don't preach"

Atypical
Micheal Jackson: "she's Out of my life"(very slow track) Madonna: "Don't cry for me Argentina'

***


```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DXaTIN6XNquoW?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

```

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DWTQllLRMgY9S?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```


### Data visualization on track-level features

```{r first_plot, echo=FALSE}
#plot(combined_corpus)


my_fisrt_plot <- combined_corpus %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major")) %>%
  ggplot(aes(x = energy, y = danceability, color = factor(mode), size = valence, label = track.name)) +
  geom_point(alpha = 0.7) +
  xlim(0, 1) +
  ylim(0, 1) +
  labs(title = "Distribution of songs of the King and Queen of pop over energy, dancebility and valence", size = "") +
  geom_rug(size = 0.1) +
  facet_wrap(~category) +
  scale_color_manual(name = "mode", values = c("lightblue", "steelblue4")) +
  #scale_fill_discrete(name = "New Legend Title") +
  theme_bw()+
  theme(title =element_text(size=15))
  

#my_fisrt_plot
p <- ggplotly(my_fisrt_plot)
p


```
***

Description plot

The following plot shows the dancebility, energy, valence and mode on track-level for the 'This is: Micheal Jackson' playlist and the 'This is: Madonna' playlist next to each other. The x- and y-axis show the energy and dancebility respectively. The size of the dots implicit the valence (musical positiveness) of a track. As you can see are the values clustered around the same place for both artist, however, the tracks of Micheal Jackson are more spread for dancebility as well as for energy. Furthermore, the artists have an equally divided ratio of major- and minor mode tracks. What stands out is that both artists have a few 'outlier' tracks, you can view which tracks these are by hovering over them. These outlier tracks will be analysed as well (To Do).



### Boxplot comparizon between Queen and King based on the mean of various features

```{r box_plot, echo=FALSE, warning=FALSE}

#str(corpus_king)

cat <- combined_corpus %>%
  select(category)

corpus_combined_select <- combined_corpus %>%
  select(danceability, energy, speechiness, acousticness, liveness, valence) %>%
  mutate(category = cat)

test <- corpus_combined_select[7] %>% pull(category)%>% pull(category)

tester <- ggplot(stack(corpus_combined_select), aes(x = values, y = ind, fill(~test))) +
  geom_boxplot() +
  facet_wrap(~corpus_combined_select[7] %>% pull(category)%>% pull(category)) +
  labs(title = "Comparison of Mean featrues between Micheal Jackson and Madonna") +
  theme_bw()+
  theme(title =element_text(size=8))

tester
#str(corpus_combined_select[8] %>% pull(category)%>% pull(category) )

```

***
Description boxplot

The previous plot showed the overall distribution on track-level features. In this boxplot the means of various musical features are compared between the King and Queen, from this plot we can make a few observations. 

- The tracks in the Micheal Jackson playlist have a higher overall valence, however, the valence in the Madonna tracks is more spread.

- The liveness between both artist is about the same, both have a few outlier tracks here, consisting out of live performances.

- The tracks of Micheal Jackson have a littel higher overall accousticness.

- Both artist have a very low overall speechiness which means that there is a lot of music in these tracks compared to spoken words.

- The overall energy is high in both playlists, where both have one distinct outlier represented by the dot on the left.

- Dancebility is also high for both artists, the dancebility of the Michael Jackson is a little higher, where the tracks of Madonna are more clustered around the mean.


### Billie Jean chromagram 

```{r echo=FALSE}

## billie_thriller_deluxe
billie_thriller_deluxe <-
  get_tidy_audio_analysis("5ChkMS8OtdzJeqyybCc9R5") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
## billie_thriller
billie_thriller <-
  get_tidy_audio_analysis("7J1uxwnxfQLu4APicE5Rnj") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
```

```{r test_chroma, echo=FALSE}

billie_thriller %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Chromagram Billie Jean Thriller album") +
  theme_minimal() +
  scale_fill_viridis_c()
```

*** 
Description chromagram

The plot above shows the chromagram for the track Billie Jean by Micheal Jackson. This version of Billie Jean is from the Thriller album in 1982. In the next section this version will be compared to the Billie Jean track from the thriller deluxe album in 2008. The euclidean is used to normalize the pitch classes. As you can see, most energy is concentrated in the pitch classes F# and C#.

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7J1uxwnxfQLu4APicE5Rnj?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```



### Dynamic Time Warping of two versions of the Billie Jean track 

```{r test_chroma2, echo=FALSE}



compmus_long_distance(
  billie_thriller_deluxe %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  billie_thriller %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "euclidean"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Billie Jean (Thriller deluxe album)", y = "Billie Jean (Thriller album)") +
  labs(title = "Distances between chroma vectors Billie Jean versions (Distance metric: euclidean)") +
  theme_minimal() +
  theme(title =element_text(size=6, face='bold')) +
  scale_fill_viridis_c(guide = NULL)

```

***
Description dynamic time warping two version

The plot besides shows dynamic time warping used to compare the two different version of Billie Jean; the Euclidean distance is used as distance metric. There is a clear diagonal line visible in the graph which shows the similarity between the two versions. This diagonal line is not very good vissible in the first 30 seconds which means that the two different version do defer a bit at that place.

### Cepstrogram of Billie Jean showing changes in timbre components 
```{r Cepstrogram, echo=FALSE}

#section, bar, beat, and tatum
bzt <-
  get_tidy_audio_analysis("7J1uxwnxfQLu4APicE5Rnj") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

```

```{r self_sim1, echo = FALSE}
bzt %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Cepstrogram of Billie Jean by Micheal Jackson", x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

```

***
Description

The cleptogram on the left shows a number of changes in the timbre components;

- At t=70 there is a shift to c02 (first chorus)
- At t=110 this shifts back to c03 (second verse)
- AT t=150 there is a shift back to c02 (second chorus), where for the rest of the song the biggest magnitude is in c02, except for a little piece around t=220 where it shifts back to c03 (musical solo).

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7J1uxwnxfQLu4APicE5Rnj?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```




### Self-Similarity Matrix of Billie Jean

```{r real-self-sim-matrix, echo = FALSE}
bzt %>%
  compmus_self_similarity(timbre, "angular") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(title = "Self-Similarity Matrix of Billie Jean by Micheal Jackson", x = "", y = "")

```

***
Description:

By looking at the self-similarity matrix while listening to the song the different transitions in the song could be distinguished:

- t=0-30 -> musical intro (only instruments).

- t=30-70 -> start of singing, first verse.

- t=70-110 -> first chorus, where the voice becomes louder.

- t=110-150 -> end of first chorus, start of second verse.

- t=150-210 -> second chorus, is also very similar to the first chorus in the ssm

- t=210-230 -> short 'musical solo', where there is no singing

- t = 230-290 -> chorus again. 

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7J1uxwnxfQLu4APicE5Rnj?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```



### Chordogram of Billie Jean.

```{r chord-billie, echo = FALSE}

Billie_jean <-
  get_tidy_audio_analysis("7J1uxwnxfQLu4APicE5Rnj") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

Billie_jean %>% 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "chebyshev"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***

Description

The chordogram on the left shows the chords that are played throughout the Billie Jean track. The Euclidean is used as method and the chebyshev as normalization, using these two metrics provides a nice visualization on the chords used; this results in a number of observations:

- F#:minor and Gb:major are the most used chords, visible by the dark blue line at these chords throughout the entire track.

- At t=70 and t=150 a vertical light blue/yellow line is visible throughout the chords,this indicates a change in the chords played at that moment. This is at the moment where the choruses start (at the line: "People always told me, "Be careful of what you do"").

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7J1uxwnxfQLu4APicE5Rnj?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```

### Track-Level Summaries key-chord-corpus

```{r key-chord-corpus, echo = FALSE}
micheal <-
  get_playlist_audio_features(
    "",
    "37i9dQZF1DXaTIN6XNquoW"
  ) %>%
  slice(1:30) %>%
  add_audio_analysis()


Mozart <-
  get_playlist_audio_features(
    "",
    "37i9dQZF1DX8qqIDAkKiQg"
  ) %>%
  slice(1:30) %>%
  add_audio_analysis()



madonna <-
  get_playlist_audio_features(
    "",
    "37i9dQZF1DWTQllLRMgY9S"
  ) %>%
  slice(1:30) %>%
  add_audio_analysis()

combined_violin <-
  micheal %>%
  mutate(Artist = "Micheal Jackson") %>%
  bind_rows(madonna %>% mutate(Artist = "Madonna") %>% bind_rows(Mozart %>% mutate(Artist = "Mozart")))

combined <-
  micheal %>%
  mutate(Artist = "Micheal Jackson") %>%
  bind_rows(madonna %>% mutate(Artist = "Madonna"))
```


```{r key-chord, echo = FALSE}
combined %>%
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) %>%
  unnest(sections) %>%
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = Artist,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Artist",
    size = "Duration (min)",
    alpha = "Volume (dBFS)"
  )
```

***

Description

The plot on the left shows the mean temp0 (in beats per minute) as well as the variation in tempo throughout the tracks. Furthermore, the length of the tracks is indicated by the size of the dots and the volume of the tracks by the transparency. From this plot we can derive a number of observation which are stated below:

- Almost all tracks, for both artists have an high volume (dBFS), this was very common in pop songs; they tried to master the tracks so that the volume is at its loudest so the music stands out. The phenomenon where the volume in tracks is increased can also be referred to as the Loudness war.

- Most tracks have a low varieties in tempo throughout the track, there is one outlier here: "Smooth criminal" by Micheal Jackson has got a high variety in the bpm throughout the track. You can check it yourself in the audio player below.

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2bCQHF9gdG5BNDVuEIEnNk?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```


- The mean tempo for both artists is clustered around 115 beats per minute which is typical for pop music (100 - 130 bpm on average). Here is also one outlier with a bpm of 188: This is the song "ABC" by the Jackson Five, this track is actually 94 bpm but it can also be used double-time at 188 BPM, the audio analyser captures the latter.

- The Madonna tracks are a little more clustered around the mean which means Madonna's tracks have less variety in tempo and especially less tempo changes in them.


### Average timbre coefficients for the Micheal Jackson and Madonna playlists


```{r violin, echo = FALSE}
combined_violin %>%
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) %>%
  select(Artist, timbre) %>%
  compmus_gather_timbre() %>%
  ggplot(aes(x = basis, y = value, fill = Artist)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(title = "Average timbre coefficients for the Micheal Jackson and Madonna playlists", x = "Spotify Timbre Coefficients", y = "", fill = "Artist")

```

***

Description

The violin plot on the side shows the different timber coefficients for the first thirty tracks in the Micheal Jackson (Blue) and Madonna (Purple) playlist, furthermore, the "This is Mozart" playlist (Yellow) is used as a third category in order to compare the two to a different musical category. A number of observation can be derived from this plot, however because the Spotify timber coefficients are rather vague no real conclusion can be drawn from this.

- The first column (c01) shows the overall loudness of the tracks, this is for Madonna as well as Micheal Jackson about the same hight, the loudness in the Mozart playlist is a bit lower.

- The biggest difference between Micheal Jackson and Madonna is in c04; the Micheal jackson tracks have a much bigger spread here.

- The Mozart tracks especially defer from the pop tracks in the timber coefficients: c02, c03 and c07.

### Conclusion & discussion 
