---
title: "King and Queen of pop"
author: "Bas de Boer"
date: "2/16/2022"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: yeti
    self_contained: false
    
---


Introduction {.storyboard}
==============================================================================================

```{r setup, include=FALSE}

library(tidyverse)
library(spotifyr)
library(forcats)
library(plotly)
library(flexdashboard)
library(compmus)
library(reshape)
library(tidymodels)
library(ggdendro)
library(heatmaply)


corpus_king <- get_playlist_audio_features("", "37i9dQZF1DXaTIN6XNquoW")
corpus_king_adapted <- corpus_king[c(-50),]

corpus_queen <- get_playlist_audio_features("", "37i9dQZF1DWTQllLRMgY9S")

best_track_king_billie <- get_track_audio_features(c("2IU9ftOgyRL2caQGWK1jjX", "5ChkMS8OtdzJeqyybCc9R5"))
best_track_queen_vergin <- get_track_audio_features(c("1C2h7mLntPSeVYciMRTF4a", "1ZPlNanZsJSPK5h9YZZFbZ"))
  
combined_corpus <-
  bind_rows(
    corpus_king %>% mutate(category = "King of pop (Michael Jackson)"),
    corpus_queen %>% mutate(category = "Queen of pop (Madonna)")
  )

combined_corpus_adapted <-
  bind_rows(
    corpus_king_adapted %>% mutate(category = "King of pop (Michael Jackson)"),
    corpus_queen %>% mutate(category = "Queen of pop (Madonna)")
  )

```

``` {r get_means_mj, include=FALSE}
mean_king <- corpus_king %>%
  summarise(
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_instrumentalness = mean(instrumentalness),
    mean_liveness = mean(liveness),
    mean_valence = mean(valence)
  )
```

``` {r get_means_madon, include=FALSE}

mean_queen <- corpus_queen %>%
  summarise(
    mean_danceability = mean(danceability),
    mean_energy = mean(energy),
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_instrumentalness = mean(instrumentalness),
    mean_liveness = mean(liveness),
    mean_valence = mean(valence)
  )
```

``` {r include=FALSE}
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  
```


```{r key-and-chord, include= FALSE}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

```

### Introduction of corpus

The corpus that will be used in this research consists out of the two Spotify "This is:" playlists. The first playlist is the "This Is Michael Jackson" playlist (containing 65 tracks), the second playlist is the "This Is Madonna" playlist (containing 59 tracks). These playlists are curated by Spotify’s Shows & Editorial team. This is a team of music experts and genres specialists from around the world, hired by Spotify to curate and manage Spotify’s own playlists. 

So, the king and the queen of pop. I choose this corpus because I think both these artists where so good in the music that it made them become legends, not only in the pop-genre, but for almost every music-lover all over the world. Regardless of the country or age of a person, everybody knows them. You can view the playlists that will be used on the right side of this page.

The groups that will be compared in this corpus are the artists. What are the features that these two artist have in common? Could this be the features that made them become legends? But also; How do songs of the king and queen of pop defer from each other? In the field of danceability, energy, tempo, instrumentalness, speechiness and more... 

The track Billie Jean by Michael Jackson will be analysed as well, there will be a play button beside to listen to the tracks while viewing the visualization. With the result that you can listen to the changes while looking at what is happening with the timbre, chroma's and chords.

One of the problems that could be encountered is that both artists make music in the same genre. This has the result that many features have approximately the same value, so that it becomes hard to distinguish between the two artists. Nevertheless, the small differences between them are also important and the artists can be compared to an other genre, which will be done with the violin plot of the timbre.


Typical:
Michael Jackson: "Billie jean", "Smooth criminal" 
Madonna: "Die another day", "Papa don't preach"

Atypical:
Michael Jackson: "she's Out of my life" (very slow track) 
Madonna: "Don't cry for me Argentina"

![michael_jackson](/home/bas/compmusic/compmusic/index_files/figure-html/micheal_jackson.jpeg){width=49%} ![madonna](/home/bas/compmusic/compmusic/index_files/figure-html/madonna.jpeg){width=49%}

***

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DXaTIN6XNquoW?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

```

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/playlist/37i9dQZF1DWTQllLRMgY9S?utm_source=generator" width="100%" height="380" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```

Data visualizations on track's audio features {.storyboard}
==============================================================================================

### Scatterplot comparison on track-level features

```{r plot_track-level_features, echo=FALSE}

track_level <- combined_corpus %>%
  mutate(mode = ifelse(mode == 0, "Minor", "Major")) %>%
  ggplot(aes(x = energy, y = danceability, color = factor(mode), size = valence, label = track.name)) +
  geom_point(alpha = 0.7) +
  xlim(0, 1) +
  ylim(0, 1) +
  labs(title = "Distribution of songs of the King and Queen of pop over energy, dancebility and valence", size = "") +
  geom_rug(size = 0.1) +
  facet_wrap(~category) +
  scale_color_manual(name = "mode", values = c("lightblue", "steelblue4")) +
  #scale_fill_discrete(name = "New Legend Title") +
  theme_bw()+
  theme(title =element_text(size=10)) 
  
# make interactive
track_level_plotly <- ggplotly(track_level)
track_level_plotly

```
***

Description scatter plot

The following plot shows the danceability, energy, valence and mode on track-level for the 'This is: Michael Jackson' playlist and the 'This is: Madonna' playlist next to each other. The x- and y-axis show the energy and danceability respectively. The size of the dots show the valence (musical positiveness) of a track. As you can see; the values are clustered around the same place for both artist, however, the tracks of Michael Jackson are more spread for dancebility as well as for energy. Furthermore, the artists have an equally divided ratio of major- and minor mode tracks. What stands out is that both artists have a few 'outlier' tracks, you can view which tracks these are by hovering over them.

In the next tab the spread of these and some additional features will be shown for both artists. 



### Boxplot comparizon between Queen and King based on the median of various features

```{r box_plot, echo=FALSE, warning=FALSE}

cat <- combined_corpus %>%
  select(category)

corpus_combined_select <- combined_corpus %>%
  select(danceability, energy, speechiness, acousticness, liveness, valence) %>%
  mutate(category = cat)

test <- corpus_combined_select[7] %>% pull(category)%>% pull(category)

box_plot_king_queen <- ggplot(stack(corpus_combined_select), aes(x = values, y = ind, fill(~test))) +
  geom_boxplot() +
  facet_wrap(~corpus_combined_select[7] %>% pull(category)%>% pull(category)) +
  labs(title = "Comparison of the five-number summary of a set features between Michael Jackson and Madonna") +
  theme_bw() +
  theme(title = element_text(size=6))

box_plot_king_queen

```

***
Description boxplot

The previous plot showed the overall distribution on track-level features. In this boxplot the medians of various musical features are compared between the King and Queen, from this plot we can make a few observations. 

- The tracks in the Michael Jackson playlist have a higher overall valence, however, the valence in the Madonna tracks is more spread.

- The liveness between both artist is about the same, both have a few outlier tracks here, consisting out of live performances.

- The tracks of Michael Jackson have a little higher overall accousticness.

- Both artist have a very low overall speechiness which means that there is a lot of music in these tracks compared to spoken words.

- The overall energy is high in both playlists, where both have one distinct outlier represented by the dot on the left.

- Danceability is also high for both artists, the dancebility of the Michael Jackson is a little higher, where the tracks of Madonna are more clustered around the mean.

How these features coherent is shown in the next tab


### Correlation matrix to show dependencies in both playlists

```{r correlation_king,  include=FALSE}
cor(corpus_king[,6:16]) %>% melt() -> melt_king
melt_king

melt_king$value[melt_king$value < 0.1 & melt_king$value > -0.1] = NA #subset the small correlations and make them NA

#Visualize correlations
cor_matrix_king <- melt_king %>% 
  ggplot(aes(x=X1, y=X2, fill=value)) + 
  geom_tile() + 
  scale_fill_gradient2(low = "red", mid = "white", high = "green") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  xlab("Var 1") +
  ylab("Var 2") +
  labs(title = "Correlations between Spotify audio features in Michael Jackson playlist") +
  theme(axis.text=element_text(size=8), title = element_text(size=8))
```

```{r correlation_queen,  include=FALSE}
cor(corpus_queen[,6:16]) %>% melt() -> melt_queen
melt_queen

melt_queen$value[melt_queen$value < 0.1 & melt_queen$value > -0.1] = NA #subset the small correlations and make them NA

#Visualize correlations
cor_matrix_queen <- melt_queen %>% 
  ggplot(aes(x=X1, y=X2, fill=value)) + 
  geom_tile() + 
  scale_fill_gradient2(low = "red", mid = "white", high = "green") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
  xlab("Var 1") +
  ylab("Var 2") +
  labs(title = "Correlations between Spotify audio features in Madonna playlist") +
  theme(axis.text=element_text(size=8), title = element_text(size=8))
```

```{r, figures-side, fig.show="hold", out.width="50%"}

cor_matrix_king
cor_matrix_queen
```

***

Description correlation matrix

The two matrices on in this tab show the overall correlation between tracks for the playlist of Madonna and Michael Jackson, where green indicates a positive correlation and red a negative one. By looking at these matrices we can deduce the following: 

- Valance has got a positive correlation with energy and dancebility, for both artists.

- Loudness has got a positive correlation with energy, for both artists.

- Accousticness has got a negative correlation with dancebility as well as energy, loudness, speechiness and valence. This is true for both artist, but the correlation is stronger in the Michael Jackson playlist.

- The Michael Jackson matrix has got more dark-red colors, indicating that there are more strongly negative correlations.

- The green diagonal line in the middle indicates the correlations with the category itself, which of course is always 1.


Analyzing Billie Jean {.storyboard}
==============================================================================================

### Billie Jean chromagram 

```{r echo=FALSE}

# billie_thriller_deluxe
billie_thriller_deluxe <-
  get_tidy_audio_analysis("5ChkMS8OtdzJeqyybCc9R5") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

# billie_thriller
billie_thriller <-
  get_tidy_audio_analysis("7J1uxwnxfQLu4APicE5Rnj") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
```

```{r Billie_jean_chromagram, echo=FALSE}

billie_thriller %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude", title = "Chromagram Billie Jean Thriller album") +
  theme_minimal() +
  scale_fill_viridis_c()
```

*** 
Description chromagram

The plot above shows the chromagram for the track Billie Jean by Michael Jackson. This version of Billie Jean is from the Thriller album in 1982. In the next tab this version will be compared to the Billie Jean track from the thriller deluxe album in 2008. The euclidean is used to normalize the pitch classes. As you can see, most energy is concentrated in the pitch classes F# and C#. 

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7J1uxwnxfQLu4APicE5Rnj?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```


### Dynamic Time Warping of two versions of the Billie Jean track 

```{r dynamic_time_warping, echo=FALSE}

compmus_long_distance(
  billie_thriller_deluxe %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  billie_thriller %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "euclidean"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Billie Jean (Thriller deluxe album)", y = "Billie Jean (Thriller album)") +
  labs(title = "Distances between chroma vectors Billie Jean versions (Distance metric: euclidean)") +
  theme_minimal() +
  theme(title =element_text(size=6, face='bold')) +
  scale_fill_viridis_c(guide = NULL)

```

***
Description dynamic time warping

The plot besides shows dynamic time warping used to compare the two different version of Billie Jean; The original one and the remastered version of the 2008 Thriller Deluxe album. The Euclidean distance is used as distance metric. There is a clear diagonal line visible in the graph which shows the similarity between the two versions. This diagonal line is not very good visible in the first 30 seconds which means that the two different version do defer a bit at that place. You can listen to both tracks and check if you can hear the difference yourself.

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7J1uxwnxfQLu4APicE5Rnj?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>

<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/5ChkMS8OtdzJeqyybCc9R5?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```




### Cepstrogram of Billie Jean showing changes in timbre components

```{r Cepstrogram, echo=FALSE}

#section, bar, beat, and tatum
Cepstrogram_billie_jean <-
  get_tidy_audio_analysis("7J1uxwnxfQLu4APicE5Rnj") %>% # Change URI.
  compmus_align(bars, segments) %>%                     # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

```

```{r Cepstrogram_billie_jean, echo = FALSE}
Cepstrogram_billie_jean %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Cepstrogram of Billie Jean by Michael Jackson", x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()

```

***
The cepstrogram on the left shows a number of changes in the timbre components, where a yellow color indicates a change in the magnitude at that component;

- At t=70 there is a shift to c02 (first chorus)
- At t=110 this shifts back to c03 (second verse)
- AT t=150 there is a shift back to c02 (second chorus), where for the rest of the song the biggest magnitude is in c02, except for a little piece around t=220 where it shifts back to c03 (musical solo).

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7J1uxwnxfQLu4APicE5Rnj?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```

### Self-Similarity Matrix of Billie Jean showing transitions in the strack

```{r real-self-sim-matrix, echo = FALSE}
Cepstrogram_billie_jean %>%
  compmus_self_similarity(timbre, "angular") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(title = "Self-Similarity Matrix of Billie Jean by Michael Jackson", x = "", y = "") +
  theme(title =element_text(size=10))

```

***

By looking at the Self-Similarity matrix while listening to the song the different transitions in the song can be distinguished:

- t=0-30 -> musical intro (only instruments).

- t=30-70 -> start of singing, first verse.

- t=70-110 -> first chorus, where the voice becomes louder.

- t=110-150 -> end of first chorus, start of second verse.

- t=150-210 -> second chorus, is also very similar to the first chorus in the ssm

- t=210-230 -> short 'musical solo', where there is no singing

- t = 230-290 -> chorus again. 

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7J1uxwnxfQLu4APicE5Rnj?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```

### Chordogram of Billie Jean.

```{r chord-billie, echo = FALSE}

Billie_jean <-
  get_tidy_audio_analysis("7J1uxwnxfQLu4APicE5Rnj") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

Billie_jean %>% 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "chebyshev"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***

Description Chordogram

The chordogram on the left shows the chords that are played throughout the Billie Jean track. The Euclidean is used as method and the chebyshev as normalization, using these two metrics provides a nice visualization of the chords throughout the track. This results in a number of observations:

- F#:minor and Gb:major are the most used chords, visible by the dark blue line at these chords throughout the entire track.

- At t=70 and t=150 a vertical light blue/yellow line is visible throughout the chords,this indicates a change in the chords played at that moment. This is at the moment where the choruses start (at the line: "People always told me, "Be careful of what you do"").

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/7J1uxwnxfQLu4APicE5Rnj?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```


Tempo {.storyboard}
==============================================================================================


### Track-Level scatterplot on tempo and standard deviation in tempo

```{r key-chord-corpus, echo = FALSE}
michael <-
  get_playlist_audio_features(
    "",
    "37i9dQZF1DXaTIN6XNquoW"
  ) %>%
  slice(1:30) %>%
  add_audio_analysis()


Mozart <-
  get_playlist_audio_features(
    "",
    "37i9dQZF1DX8qqIDAkKiQg"
  ) %>%
  slice(1:30) %>%
  add_audio_analysis()

madonna <-
  get_playlist_audio_features(
    "",
    "37i9dQZF1DWTQllLRMgY9S"
  ) %>%
  slice(1:30) %>%
  add_audio_analysis()

combined_violin <-
  michael %>%
  mutate(Artist = "Michael Jackson") %>%
  bind_rows(madonna %>% mutate(Artist = "Madonna") %>% bind_rows(Mozart %>% mutate(Artist = "Mozart")))

combined <-
  michael %>%
  mutate(Artist = "Michael Jackson") %>%
  bind_rows(madonna %>% mutate(Artist = "Madonna"))
```


```{r key-chord, echo = FALSE}
combined %>%
  mutate(
    sections =
      map(
        sections,                                    # sections or segments
        summarise_at,
        vars(tempo, loudness, duration),             # features of interest
        list(section_mean = mean, section_sd = sd)   # aggregation functions
      )
  ) %>%
  unnest(sections) %>%
  ggplot(
    aes(
      x = tempo,
      y = tempo_section_sd,
      colour = Artist,
      alpha = loudness
    )
  ) +
  geom_point(aes(size = duration / 60)) +
  geom_rug() +
  theme_minimal() +
  ylim(0, 5) +
  labs(
    x = "Mean Tempo (bpm)",
    y = "SD Tempo",
    colour = "Artist",
    size = "Duration (min)",
    alpha = "Volume (dBFS)",
    title = "Scatterplot on track-level tempo"
  )
```

***

Description scatterplot

The plot on the left shows the mean tempo (in beats per minute) as well as the variation in tempo throughout the tracks. Furthermore, the length of the tracks is indicated by the size of the dots and the volume of the tracks by the transparency. From this plot we can derive a number of observation which are stated below:

- Almost all tracks, for both artists have an high volume (dBFS), this was very common in pop songs; they tried to master the tracks so that the volume is at its loudest with the result that the music stands out. The phenomenon where the volume in tracks is increased can also be referred to as the Loudness war.

- Most tracks have a low varieties in tempo throughout the track, there is one outlier here: "Smooth criminal" by Michael Jackson has got a high variety in the bpm throughout the track. You can check it yourself in the audio player below.

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2bCQHF9gdG5BNDVuEIEnNk?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```

- The mean tempo for both artists is clustered around 120 beats per minute which is typical for pop music (100 - 130 bpm on average). Here is also one outlier with a bpm of 188: This is the song "ABC" by the Jackson Five, this track is actually 94 bpm but it can also be used double-time at 188 BPM, the audio analyser captures the latter. In the next tab the cyclic tempogram of this track will be shows.

- The Madonna tracks are a little more clustered around the mean which means Madonna's tracks have less variety in tempo and especially less tempo changes in the tracks.


### Cyclic tempogram of ABC based on Novelty function
```{r novelty, echo = FALSE, eval = FALSE}
abc <- get_tidy_audio_analysis("2VCUcIPY3i6pv9Nu9Vg6k3")

abc %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)", title =  "Cyclic tempogram of ABC") +
  theme_classic()
```
![bpm](/home/bas/compmusic/compmusic/index_files/figure-html/novelty-1.png){width=100%}

*** 

Description

Cyclic tempogram of the track ABC by the Jackson 5. In the previous graph this tracks was labeled as a track with a tempo of 188, however, this cyclic tempogram clearly shows that this is not the case. The tempo throughout the track is about 94 BPM which means that the graph in the previous tab indeed showed the double-time beats for this track. You can listen to the track yourself and tab along with [this site](https://www.beatsperminuteonline.com).

```{=html}
<iframe style="border-radius:12px" src="https://open.spotify.com/embed/track/2VCUcIPY3i6pv9Nu9Vg6k3?utm_source=generator" width="100%" height="80" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>
```


### Density chart of tempo distribution in both playlists

```{r density_chart_bpm, echo  = FALSE}
combined_corpus$artist <-  sapply(combined_corpus$track.album.artists, function(mat) unlist(mat)[3])

test <- combined_corpus %>%
  ggplot(aes(x <- tempo, group = category, fill = category)) + 
  geom_density(alpha=.4) +
  labs(title = "Density chart of tempo for the Michael Jackson and Madonna playlists") +
  theme(title = element_text(size=8))

ggplotly(test)
```


***

Description Density chart

This density graph shows the distribution of tempo in the playlists of the King and Queen to get a better visualization of the distribution of the tempo from the scatterplot two tabs to the left. Both playlists have a peak around 120 bpm, this is a normal mean in bpm for pop.

What also stands out is that the Michael Jackson curve is a bit more flattened, indicating that the tempo in this playlist is a bit less clustered around one spot, an observation which we could also see in the scatterplot.

Furthermore, there is still some density in the graph around 200 bpm for Michael Jackson, this is caused by a mistake in the process of counting the beats per minute. An example of this outlier is shows in the previous tab.


Timbre {.storyboard}
==============================================================================================

### Average timbre coefficients for the Michael Jackson and Madonna playlists compared to a different genre


```{r violin, echo = FALSE}
combined_violin %>%
  mutate(
    timbre =
      map(
        segments,
        compmus_summarise,
        timbre,
        method = "mean"
      )
  ) %>%
  select(Artist, timbre) %>%
  compmus_gather_timbre() %>%
  ggplot(aes(x = basis, y = value, fill = Artist)) +
  geom_violin() +
  scale_fill_viridis_d() +
  labs(title = "Average timbre coefficients for the Michael Jackson, Madonna and Mozart playlists", x = "Spotify Timbre Coefficients", y = "", fill = "Artist") +
  theme(title = element_text(size=8))

```

***

Description violin plot

The violin plot on the side shows the different timber coefficients for the first thirty tracks in the Michael Jackson (Blue) and Madonna (Purple) playlist, furthermore, the "This is Mozart" playlist (Yellow) is used as a third category in order to compare the two to a different musical genre. A number of observation can be derived from this plot, however because the Spotify timber coefficients are rather vague no real conclusion can be drawn from this.

- The first column (c01) shows the overall loudness of the tracks, this is for Madonna as well as Michael Jackson about the same height, the loudness in the Mozart playlist is a bit lower.

- The biggest difference between Michael Jackson and Madonna is in c04; the Michael Jackson tracks have a much bigger spread here. All the other timbre coefficients are more or less the same for Michael Jackson and Madonna.

- The Mozart tracks especially defer from the pop tracks in the timber coefficients: c02, c03 and c07.


Classification and clustering {.storyboard}
==============================================================================================

### Classification


```{r importance, echo = FALSE, eval = FALSE}

king_queen_features <-
  combined_corpus_adapted %>%  # For your portfolio, change this to the name of your corpus.
  add_audio_analysis() %>% 
  mutate(
    playlist = factor(category),
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(
        segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean",
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))


king_queen_recipe <-
  recipe(
    playlist ~
      danceability +
      energy +
      loudness +
      speechiness +
      acousticness +
      instrumentalness +
      liveness +
      valence +
      tempo +
      duration +
      C + `C#|Db` + D + `D#|Eb` +
      E + `F` + `F#|Gb` + G +
      `G#|Ab` + A + `A#|Bb` + B,
    data = king_queen_features,          # Use the same name as the previous block.
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())      # Converts to z-scores.
# step_range(all_predictors())    # Sets range to [0, 1].


indie_cv <- king_queen_features %>% vfold_cv(5)


knn_model <-
  nearest_neighbor(neighbors = 1) %>%
  set_mode("classification") %>% 
  set_engine("kknn")
indie_knn <- 
  workflow() %>% 
  add_recipe(king_queen_recipe) %>% 
  add_model(knn_model) %>% 
  fit_resamples(
    indie_cv, 
    control = control_resamples(save_pred = TRUE)
  )



indie_knn %>% get_conf_mat() %>% autoplot(type = "heatmap")

indie_knn %>% get_pr()

forest_model <-
  rand_forest() %>%
  set_mode("classification") %>% 
  set_engine("ranger", importance = "impurity")
king_queen_forest <- 
  workflow() %>% 
  add_recipe(king_queen_recipe) %>% 
  add_model(forest_model) %>% 
  fit_resamples(
    indie_cv, 
    control = control_resamples(save_pred = TRUE)
  )

king_queen_forest %>% get_conf_mat() %>% autoplot(type = "heatmap")
king_queen_forest %>% get_pr()


workflow() %>% 
  add_recipe(king_queen_recipe) %>% 
  add_model(forest_model) %>% 
  fit(king_queen_features) %>% 
  pluck("fit", "fit", "fit") %>%
  ranger::importance() %>% 
  enframe() %>% 
  mutate(name = fct_reorder(name, value)) %>% 
  ggplot(aes(name, value)) + 
  geom_col() + 
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Importance")


king_queen_features %>%
  ggplot(aes(x = acousticness, y = danceability, colour = playlist, size = valence)) +
  geom_point(alpha = 0.8) +
  scale_color_viridis_d() +
  labs(
    x = "acousticness",
    y = "danceability",
    size = "valence",
    colour = "Playlist"
  )


```

![bpm](/home/bas/compmusic/compmusic/index_files/figure-html/importance-1.png){width=45%} ![bpm](/home/bas/compmusic/compmusic/index_files/figure-html/importance-2.png){width=45%}


***

A k-nearest neighbour classifier has been used to check how well the tracks of Michael Jackson and Madonna can be classified correctly, you can see the results of this classifier on the left. The results of this classifier are poor, the precision is around 0.54 and the recall around 0.49, which means that the classes are almost randomly ordered and not a good distinction can be made.

When using the more powerful random Forrest algorithm (plot on the right) the precision and recall become a little higher, around 0.69 and 0.70 respectively, but this is still not a very good score, meaning that it is quite hard to use the Spotify API features to classify if a track is made by the King- or the Queen of pop.



### Clustering

![bpm](/home/bas/compmusic/compmusic/index_files/figure-html/importance-3.png){width=48%} ![bpm](/home/bas/compmusic/compmusic/index_files/figure-html/importance-4.png){width=48%}

***

The bar plot on the left gives us a ranking of feature importance, which is a measure of how useful each feature in the recipe was for distinguishing the ground-truth classes. This ranking has been made based on Random forests; a powerful variant of the decision-tree algorithm. As you can see is the chords G# the most important feature to distinguish between the King and Queen of pop, followed by accousticness and danceability. 

The Scatter plot on the right shows a distribution of tracks based on the features danceability and accousticness. There can not be a very clear border between the two categories, and most of the tracks are still clustered around the same place for both artists. This means that, even when the most important features are used, not a clear classification between the two artists can be made. This makes sense because both artists belong to the same genre and have many values of features in common, as can been seen in almost all previous plots.




Conclusion {.storyboard}
==============================================================================================

### Conclusion & discussion 

What stands out in the the analysed Spotify features for both Michael Jackson as well as Madonna is that the tracks of both artists have a high average dancebility and energy. Furthermore, the average tempo for both artists is around 120 beats per minute, this is around the average for pop songs (100-130 bpm). This high energy and dancebility are of course important in the success of both artists, especially in pop genre dancing and energy is very important. However, what I can conclude form this analyses is that the features of the tracks itself where not the most important factor in the success of both artists. It was the the pioneering innovation of subjects and song-texts, because pop is more than just music. It’s about the image, the publicity, the collaborations, the hit singles, the cultural and sociopolitical messages behind the songs, and the theatrical extravaganzas the artists produce on stage. 

For Michael Jackson it was his groundbreaking creativity. For instance, his theatrical extravaganzas with his incredible dance moves and on top of that he brought messages to the world with his songs that no artist has addressed before. He challenged the country’s ideas of how an African-American man should behave and be seen, topics like differences in race, sexuality and racism and where adressed in his songs. This incredible creativity along with the high energy and dancebility in his songs made him not only the perfect pop-star but the pop-legend.

Madonna reinvented the way women were perceived in pop culture, forever pushing the sexual boundaries on stage and on screen. Besides here great music, there was also an message behind most of her songs, for instance; "Like a prayer" where she kinda offends the christian church by singing a song of a girl in love with god. She provokes people and makes them think, an aspect also very familiar to the Michael Jackson songs. But, besides her music career Madonna also was a artist, dancer, musician, vocalist, director, producer, actress, entrepreneur, philanthropist and children’s book writer. Making her a great exemplary role for women in that time. 

Concluding, both artist have very similar means for most Spotify features, where especially energy and dancebility stands out. Michael Jackson's tracks are a bit more varied looking at the spread in the graphs. But, what made both these artist become legends is not only the sounds and features of their music, it was the groundbreaking message in the tracks and their creativity and personality, not only on stage but also behind the scenes. Causing theses names to be remembered for a very long time.


***

![michael_jackson_2](/home/bas/compmusic/compmusic/index_files/figure-html/Michael_Jackson_2.jpg){width=100%}
![madonna](/home/bas/compmusic/compmusic/index_files/figure-html/Madonna_2.jpg){width=100%}


